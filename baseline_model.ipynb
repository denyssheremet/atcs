{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3e2bab-46c3-4596-b02f-1d591772bca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq datasets torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11e252c-0a67-499a-b288-b65f2a2d27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset, get_dataset_split_names\n",
    "ds_train, ds_val, ds_test = load_dataset(\"stanfordnlp/snli\", split=['train', 'validation', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c911ea5-55ff-44fb-9381-d8eccf5979b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import GloVe, build_vocab_from_iterator\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b65a76b-203f-450d-af74-eb237352a1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_item(item):\n",
    "    item['premise'] = word_tokenize(item['premise'].lower())\n",
    "    item['hypothesis'] = word_tokenize(item['hypothesis'].lower())\n",
    "    return item\n",
    "\n",
    "# # load the datasets or if not available, preprocess them again. \n",
    "# import pickle\n",
    "# try: \n",
    "#     with open('all_ds_tok.pickle', 'rb') as handle:\n",
    "#         all_ds_tok = pickle.load(handle)\n",
    "    \n",
    "#     ds_train_tok = all_ds_tok[0]\n",
    "#     ds_val_tok   = all_ds_tok[1]\n",
    "#     ds_test_tok  = all_ds_tok[2]\n",
    "# except: \n",
    "ds_train_tok = ds_train.map(preprocess_item)\n",
    "ds_val_tok = ds_val.map(preprocess_item)\n",
    "ds_test_tok = ds_test.map(preprocess_item)\n",
    "all_ds_tok = [ds_train_tok, ds_val_tok, ds_test_tok]\n",
    "\n",
    "# with open('all_ds_tok.pickle', 'wb') as handle:\n",
    "#     pickle.dump(all_ds_tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d63066-6036-4ba1-854c-41ab3dab6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the unique tokens\n",
    "def get_all_unique_toks(all_ds_tok):\n",
    "    all_unique_toks = set()\n",
    "    for ds in all_ds_tok:\n",
    "        for item in tqdm.tqdm(ds):\n",
    "            for key in ['premise', 'hypothesis']:\n",
    "                for tok in item[key]:\n",
    "                    if not tok in all_unique_toks:\n",
    "                        all_unique_toks.add(tok)\n",
    "    return all_unique_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f789ec-a3fa-46c8-8dc4-556b473b95e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "try: \n",
    "    with open('all_unique_toks.pickle', 'rb') as handle:\n",
    "        all_unique_toks = pickle.load(handle)\n",
    "except: \n",
    "    all_unique_toks = get_all_unique_toks(all_ds_tok)\n",
    "    with open('all_unique_toks.pickle', 'wb') as handle:\n",
    "        pickle.dump(all_unique_toks, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756abbfa-1a42-41e8-8e54-d45b4057311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = GloVe(name='840B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d89eb9-36f8-4710-97f3-ef95d9010cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab = build_vocab_from_iterator(\n",
    "    [iter(all_unique_toks)],\n",
    "    specials=['<unk>'],\n",
    "    special_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8decea-dfc6-40d5-93b3-6005ac57b587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37210, 37211)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_unique_toks), len(glove_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e29202d9-fe39-43de-8b5a-ba409d1ea2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_index(item):\n",
    "    item['premise'] = glove_vocab.lookup_indices(item['premise'])    \n",
    "    item['hypothesis'] = glove_vocab.lookup_indices(item['hypothesis'])\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2de97277-d106-4505-b1c1-3b75334352ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets or if not available, convert to indices again. \n",
    "import pickle\n",
    "try: \n",
    "    with open('all_ds_prep.pickle', 'rb') as handle:\n",
    "        all_ds_prep = pickle.load(handle)\n",
    "    ds_train_prep = all_ds_prep[0]\n",
    "    ds_val_prep = all_ds_prep[1]\n",
    "    ds_test_prep = all_ds_prep[2]\n",
    "except: \n",
    "    # map to index and remove items with label -1\n",
    "    ds_train_prep = ds_train_tok.map(token_to_index).filter(lambda x: x['label'] >= 0)\n",
    "    ds_val_prep   = ds_val_tok.map(token_to_index).filter(lambda x: x['label'] >= 0)\n",
    "    ds_test_prep  = ds_test_tok.map(token_to_index).filter(lambda x: x['label'] >= 0)\n",
    "    all_ds_prep = [ds_train_prep, ds_val_prep, ds_test_prep]\n",
    "    \n",
    "    with open('all_ds_prep.pickle', 'wb') as handle:\n",
    "        pickle.dump(all_ds_prep, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5efa76a3-ea09-4c58-afa8-701c31eba9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find longest sentence\n",
    "# max_sent_len = -1\n",
    "# for ds in all_ds_prep:\n",
    "#     for item in ds:\n",
    "#         max_sent_len = max(\n",
    "#             max_sent_len,\n",
    "#             len(item['premise']),\n",
    "#             len(item['hypothesis'])\n",
    "#         )\n",
    "# print(max_sent_len)\n",
    "\n",
    "# max sentence length = 82\n",
    "max_sent_len = 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bad6b494-7aca-4ddb-9028-c5fe4b9e063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    # first find the longest sentences in the batch\n",
    "    max_p, max_h = -1, -1\n",
    "    for d in data:\n",
    "        max_p = max(max_p, len(d['premise']))\n",
    "        max_h = max(max_h, len(d['hypothesis']))\n",
    "    \n",
    "    # pad all sentences to same length\n",
    "    bs = len(data)\n",
    "    batch_p = torch.zeros((bs,max_p), dtype=torch.int32)\n",
    "    batch_h = torch.zeros((bs,max_h), dtype=torch.int32)\n",
    "    batch_l = torch.zeros((bs), dtype=torch.int64)\n",
    "    \n",
    "    for i, d in enumerate(data):\n",
    "        batch_p[i, 0:len(d['premise'])] = torch.tensor(d['premise'])\n",
    "        batch_h[i, 0:len(d['hypothesis'])] = torch.tensor(d['hypothesis'])\n",
    "        batch_l[i] = d['label']\n",
    "        \n",
    "    return batch_p, batch_h, batch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b5691e-3018-44e5-91b1-597048773fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "bs = 64\n",
    "\n",
    "train_loader = DataLoader(ds_train_prep, collate_fn=collate_fn, batch_size=bs, shuffle=True)\n",
    "val_loader   = DataLoader(ds_val_prep, collate_fn=collate_fn, batch_size=bs, shuffle=True)\n",
    "test_loader  = DataLoader(ds_test_prep, collate_fn=collate_fn, batch_size=bs, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e4a9de4-525e-498d-b462-5d535b88f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create the embedding layer and freeze the weights\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(\n",
    "            glove_vectors.get_vecs_by_tokens([\"<unk>\", *list(all_unique_toks)]),\n",
    "            freeze=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # unpack\n",
    "        premises, hypotheses, labels = x\n",
    "        # embed both the premise and hypothesis separately\n",
    "        premises = self.embedding_layer(premises)\n",
    "        hypotheses = self.embedding_layer(hypotheses)\n",
    "        return (premises, hypotheses, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "def3fcaa-92d8-48ce-aee1-7221e503f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    # the baseline encoder takes the average across word embeddings in a sentence\n",
    "    def forward(self, x):\n",
    "        # unpack\n",
    "        premises, hypotheses, labels = x\n",
    "        premises = premises.mean(axis=1)\n",
    "        hypotheses = hypotheses.mean(axis=1)\n",
    "        return (premises, hypotheses, labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "05a3b21b-d511-4b64-9c17-f3044de90436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinationModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    # takes u and v (premise and hypothesis) \n",
    "    # and returns (u,v | u-v | u*v)\n",
    "    def forward(self, x):\n",
    "        # unpack\n",
    "        u, v, labels = x\n",
    "        out = torch.hstack([u,v,u-v,u*v])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ff48038-4291-47cc-afdb-a6c95a83c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3420fe4a-c681-479d-94bf-e946b1bd2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, encoder, mlp):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList([\n",
    "            EmbeddingModule(),\n",
    "            encoder,\n",
    "            CombinationModule(),\n",
    "            mlp\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for m in self.model:\n",
    "            x = m(x)\n",
    "        return x\n",
    "    \n",
    "    def p(self):\n",
    "        print(self.model[1:])\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        torch.save(self.model[1:], filename)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        layers = torch.load(filename)\n",
    "        for i, l in enumerate(layers):\n",
    "            self.model[i+1] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8afd7f87-375f-46f4-8bf1-091df0761db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    losses, accs = [], []\n",
    "    \n",
    "    for premises, hypotheses, targets in tqdm(loader):\n",
    "\n",
    "        premises = premises.cuda()\n",
    "        hypotheses = hypotheses.cuda()\n",
    "        targets = targets.cuda()\n",
    "            \n",
    "        predictions = model((premises, hypotheses, targets))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_module(predictions, targets)\n",
    "        acc = (predictions.argmax(axis=-1) == targets).float().mean()\n",
    "        \n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss = torch.tensor(losses).mean()\n",
    "    train_acc = torch.tensor(accs).mean()\n",
    "    return train_loss, train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9faf9e98-de0e-4121-b891-2c7f2a7c1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure accuracy\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_correct = 0.\n",
    "    total = 0.\n",
    "    bs = loader.batch_size\n",
    "    \n",
    "    \n",
    "    for premises, hypotheses, targets in loader:\n",
    "        premises = premises.cuda()\n",
    "        hypotheses = hypotheses.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model((premises, hypotheses, targets)).argmax(axis=-1)\n",
    "        total_correct += (predictions==targets).float().sum()\n",
    "        total += bs\n",
    "        \n",
    "    acc = total_correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a85d8c8f-9f03-4782-bd32-a61c64e462d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer, new_lr):\n",
    "    # update the learning rate for the optimizer\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = new_lr\n",
    "\n",
    "def train_loop(model, optimizer, train_loader, val_loader, checkpoint_path):\n",
    "    lr = 0.1\n",
    "    last_acc, best_acc = -1, -1\n",
    "    epoch = -1\n",
    "    \n",
    "    while lr > 1e-5:\n",
    "        epoch += 1\n",
    "        print(f'lr: {lr}')\n",
    "        #train and evaluate\n",
    "        train_loss, train_acc = train_epoch(model, train_loader)\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Acc/train\", train_acc, epoch)\n",
    "\n",
    "        acc = evaluate(model, val_loader)\n",
    "        writer.add_scalar(\"Acc/eval\", acc, epoch)\n",
    "        print(f'acc: {acc}')\n",
    "        \n",
    "        \n",
    "        # learning rate decay\n",
    "        lr = lr * 0.99\n",
    "        update_lr(optimizer, lr)\n",
    "        \n",
    "        # save best checkpoint if necessary\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            model.save_model(checkpoint_path)\n",
    "        \n",
    "        # if val acc goes down, divide lr by 5\n",
    "        if acc < last_acc:\n",
    "            lr = lr / 5.\n",
    "            update_lr(optimizer, lr)\n",
    "            \n",
    "        last_acc = acc\n",
    "    writer.flush()\n",
    "    \n",
    "    # load the best model\n",
    "    model.load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0c7b91c-d443-477c-b739-aba59a4b8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(baseline_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2d622f2e-b58b-4c0a-85af-f10c2bb9f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(300,300, batch_first=True)\n",
    "    \n",
    "    # the baseline encoder takes the average across word embeddings in a sentence\n",
    "    def forward(self, x):\n",
    "        # unpack\n",
    "        premises, hypotheses, labels = x\n",
    "        premises = self.lstm.forward(premises)[1][0][0]\n",
    "        hypotheses =  self.lstm.forward(hypotheses)[1][0][0]\n",
    "        return (premises, hypotheses, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f78284fc-9940-41cd-8c1b-8807d38bcc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(300,300, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    # the baseline encoder takes the average across word embeddings in a sentence\n",
    "    def forward(self, x):\n",
    "        # unpack\n",
    "        premises, hypotheses, labels = x\n",
    "        \n",
    "        premises = self.lstm.forward(premises)[1][0]\n",
    "        premises = torch.hstack([premises[0], premises[1]])\n",
    "        \n",
    "        hypotheses =  self.lstm.forward(hypotheses)[1][0]\n",
    "        hypotheses = torch.hstack([hypotheses[0], hypotheses[1]])\n",
    "        \n",
    "        return (premises, hypotheses, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "86a54ea5-1858-46c2-bb93-5ed17ffffb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledBiLSTMEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(300,300, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    # the baseline encoder takes the average across word embeddings in a sentence\n",
    "    def forward(self, x):\n",
    "        # unpack\n",
    "        premises, hypotheses, labels = x\n",
    "        \n",
    "        premises = self.lstm.forward(premises)[0].max(dim=1)[0]\n",
    "        hypotheses =  self.lstm.forward(hypotheses)[0].max(dim=1)[0]\n",
    "    \n",
    "        return (premises, hypotheses, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5a86cb20-0f6c-45af-9ccc-c343ba086bec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "FullModel(\n",
      "  (model): ModuleList(\n",
      "    (0): EmbeddingModule(\n",
      "      (embedding_layer): Embedding(37211, 300)\n",
      "    )\n",
      "    (1): PooledBiLSTMEncoder(\n",
      "      (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
      "    )\n",
      "    (2): CombinationModule()\n",
      "    (3): MLP(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=2400, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=3, bias=True)\n",
      "        (3): Softmax(dim=1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "lr: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 119.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.4104098975658417\n",
      "lr: 0.099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 123.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.4139610528945923\n",
      "lr: 0.09801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 122.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.4126420319080353\n",
      "lr: 0.01940598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 82/154 [00:00<00:00, 122.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [223], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# train loop\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/baseline_model.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn [38], line 15\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, optimizer, train_loader, val_loader, checkpoint_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#train and evaluate\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss, epoch)\n\u001b[1;32m     17\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcc/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_acc, epoch)\n",
      "Cell \u001b[0;32mIn [29], line 7\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m losses, accs \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m premises, hypotheses, targets \u001b[38;5;129;01min\u001b[39;00m tqdm(loader):\n\u001b[1;32m      9\u001b[0m     premises \u001b[38;5;241m=\u001b[39m premises\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     10\u001b[0m     hypotheses \u001b[38;5;241m=\u001b[39m hypotheses\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [13], line 16\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[1;32m     15\u001b[0m     batch_p[i, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mlen\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremise\u001b[39m\u001b[38;5;124m'\u001b[39m])] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremise\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mbatch_h\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhypothesis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhypothesis\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m     batch_l[i] \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_p, batch_h, batch_l\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# setup tensorboard\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "model = FullModel(\n",
    "    # BaselineEncoder(),\n",
    "    PooledBiLSTMEncoder(),\n",
    "    MLP(in_dim=2400),\n",
    ").cuda()\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "\n",
    "# train loop\n",
    "train_loop(\n",
    "    model, \n",
    "    optimizer, \n",
    "    test_loader, \n",
    "    val_loader,\n",
    "    \"checkpoints/baseline_model.pickle\",\n",
    ")\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2519cba6-bd8c-46d6-a714-10e0782a2ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  8,  9, 10, 11],\n",
       "        [ 4,  5,  6,  7, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange((16)).reshape(2,2,4)\n",
    "print(t)\n",
    "torch.hstack([t[0],t[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf796d-9469-460f-88d1-b9be36d3bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model[1:], \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5b37601-5430-47c2-88f5-d2c45c68b139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6118, device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e341413-a8bd-473e-b664-0438eb291bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('checkpoints/baseline_model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e04322ea-a900-43df-a801-c809091dd4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2926\n",
      "-rw-r--r--  1 root root       6 Apr 20 12:51 README.md\n",
      "-rw-r--r--  1 root root    5324 Apr 21 20:53 all_ds_prep.pickle\n",
      "-rw-r--r--  1 root root    4503 Apr 21 15:26 all_ds_tok.pickle\n",
      "-rw-r--r--  1 root root  392430 Apr 20 15:47 all_unique_toks.pickle\n",
      "-rw-r--r--  1 root root 2469544 Apr 21 20:38 baseline_model_MLP.pickle\n",
      "drwxr-xr-x  2 root root       2 Apr 21 21:59 checkpoints\n",
      "-rw-r--r--  1 root root   26449 Apr 21 22:00 preprocessing-Copy1.ipynb\n",
      "-rw-r--r--  1 root root   36479 Apr 21 21:31 preprocessing.ipynb\n",
      "drwxr-xr-x 23 root root      21 Apr 21 21:39 runs\n",
      "drwxr-xr-x  2 root root       0 Apr 21 21:57 saved_models\n",
      "-rw-r--r--  1 root root   58619 Apr 20 15:06 test.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b96da1a-5ef1-413b-bb80-f3f167e13b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.model[1:], 'test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "285943e3-7180-4931-985c-3edcbb2f8f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6118, device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_model('saved_models/baseline_model.pickle')\n",
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5158d6-4c05-46ce-8c0b-e96c858480b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
