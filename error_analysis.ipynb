{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ea7eb9-c465-4b75-9a2e-0b628083d2dc",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "We first set up the functions, and then do the analysis. \n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b279ca9-214c-4bb2-8dd4-55271ead53d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from preprocessing import preprocess_item, get_unique_tokens, token_to_index, setup_glove\n",
    "from modules import FullModel, EmbeddingModule, CombinationModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c6eb35-2011-4c18-8537-374d16624176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading glove vectors...\n",
      "loading glove vocab...\n"
     ]
    }
   ],
   "source": [
    "# setup glove\n",
    "all_unique_toks = get_unique_tokens(None, None, None) # can be None because unique_tokens are loaded from pickle\n",
    "glove_vectors, glove_vocab = setup_glove(all_unique_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e984fa3a-307f-4128-b8e3-5fb181a6a46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \n",
      "{'premise': 'Two men sitting in the sun', 'hypothesis': 'Nobody is sitting in the shade', 'label': 1}\n",
      "\n",
      "preprocessed:\n",
      "(tensor([[34378, 19979, 29446, 16457, 32987, 31890]], device='cuda:0'), tensor([[21583, 17072, 29446, 16457, 32987, 28751]], device='cuda:0'), tensor([1], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "def build_example(premise, hypothesis, label):\n",
    "    # make example\n",
    "    example_text = {\n",
    "        'premise': premise,\n",
    "        'hypothesis': hypothesis,\n",
    "        'label': label\n",
    "    }\n",
    "    # tokenize it\n",
    "    example_tok = example_text.copy()\n",
    "    example_tok = preprocess_item(example_tok)\n",
    "    example_prep = token_to_index(example_tok, glove_vocab)\n",
    "    \n",
    "    prem_tok = torch.tensor(example_prep['premise']).cuda().unsqueeze(0)\n",
    "    hyp_tok = torch.tensor(example_prep['hypothesis']).cuda().unsqueeze(0)\n",
    "    lab_tok = torch.tensor(example_prep['label']).cuda().unsqueeze(0)\n",
    "    \n",
    "    return example_text, (prem_tok, hyp_tok, lab_tok)\n",
    "    \n",
    "\n",
    "premise = \"Two men sitting in the sun\"\n",
    "hypothesis = \"Nobody is sitting in the shade\"\n",
    "# 0: entailment, 1: neutral, 2: contradiction\n",
    "label = 1 \n",
    "\n",
    "example_text, example_prep = build_example(premise, hypothesis, label)\n",
    "print(f'text: \\n{example_text}')\n",
    "print(f'\\npreprocessed:\\n{example_prep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5580641-af96-4848-a683-5d0283594ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = FullModel(\n",
    "    EmbeddingModule(glove_vectors, all_unique_toks),\n",
    "    None,\n",
    "    CombinationModule(),\n",
    "    None\n",
    ").cuda()\n",
    "model.load_model(\"pooledbilstm.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c660041b-bfb3-4a14-b204-56772f173a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(pred, targ=None):\n",
    "    d = ['Entailment', 'Neutral', 'Contradiction']\n",
    "    s = f\"Prediction:   {d[pred]}\"\n",
    "    if not targ == None:\n",
    "        s += f'\\nGround truth: {d[targ]}'\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a504a2-6e90-4d8c-be0a-5afd8b6e084a",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "First, let the model predict the first example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0da4df-f0a8-416e-a21d-3c83a7e38657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Two men sitting in the sun',\n",
       " 'hypothesis': 'Nobody is sitting in the shade',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "915a485a-7e55-4544-8cc0-8978a3c71f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:878: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:   Contradiction\n",
      "Ground truth: Neutral\n"
     ]
    }
   ],
   "source": [
    "# test the prediction\n",
    "pred = model(example_prep).argmax()\n",
    "print_prediction(pred, example_prep[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677037a-efd8-4836-9ec6-54f20cdbc584",
   "metadata": {},
   "source": [
    "As expected (from the assignment text), the model predicts Contradiction instead of Neutral. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f37b07e-f023-4a17-8362-4bd757212cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:   Contradiction\n",
      "Ground truth: Neutral\n"
     ]
    }
   ],
   "source": [
    "ex2, ex2_prep = build_example(\n",
    "    \"A man is walking a dog\",\n",
    "    \"No cat is outside\",\n",
    "    1\n",
    ")\n",
    "pred2 = model(ex2_prep).argmax()\n",
    "print_prediction(pred, ex2_prep[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ea59f-5593-410c-990d-8107da4dc901",
   "metadata": {},
   "source": [
    "For the second example, we also see that the model thinks it is a contradication. \n",
    "My hypothesis is that this is because the encoding of \"dog\" and \"cat\" are similar for the model. To test this, I will change \"cat\" for \"car\" and \"bird\" and see if the model still makes a mistake. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f8bf6c-96f4-4766-a7a6-8cf1a1092611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car:\n",
      "Prediction:   Contradiction\n",
      "Ground truth: Neutral\n",
      "\n",
      "bird:\n",
      "Prediction:   Contradiction\n",
      "Ground truth: Neutral\n"
     ]
    }
   ],
   "source": [
    "ex3, ex3_prep = build_example(\n",
    "    \"A man is walking a dog\",\n",
    "    \"No car is outside\",\n",
    "    1\n",
    ")\n",
    "ex4, ex4_prep = build_example(\n",
    "    \"A man is walking a dog\",\n",
    "    \"No bird is outside\",\n",
    "    1\n",
    ")\n",
    "\n",
    "print(\"car:\")\n",
    "print_prediction(model(ex3_prep).argmax(), 1)\n",
    "print(\"\\nbird:\")\n",
    "print_prediction(model(ex4_prep).argmax(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01595ee4-319f-401d-9b67-15025b9f59e0",
   "metadata": {},
   "source": [
    "The model still predicts Contradiction for both \"Car\" and \"Bird\". Maybe the model always predicts contradiction? We invert the hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "508bd1e9-20e8-41a5-8bc2-608619f1f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man outside:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Entailment\n",
      "\n",
      "cat:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n",
      "\n",
      "hamburger stand:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n"
     ]
    }
   ],
   "source": [
    "_, ex5_prep = build_example(\n",
    "    \"A man is walking a dog\",\n",
    "    \"A man is outside\",\n",
    "    0\n",
    ")\n",
    "_, ex6_prep = build_example(\n",
    "    \"A man is walking a dog\",\n",
    "    \"A cat is outside\",\n",
    "    1\n",
    ")\n",
    "\n",
    "_, ex7_prep = build_example(\n",
    "    \"A man is walking a dog\",\n",
    "    \"A hamburger stand is outside\",\n",
    "    1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"man outside:\")\n",
    "print_prediction(model(ex5_prep).argmax(), 0)\n",
    "print(\"\\ncat:\")\n",
    "print_prediction(model(ex6_prep).argmax(), 1)\n",
    "print(\"\\nhamburger stand:\")\n",
    "print_prediction(model(ex7_prep).argmax(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a679452-506f-46f3-bae5-a8c01102e91a",
   "metadata": {},
   "source": [
    "The model correctly predicts Entailment about whether the man is outside, but it also predicts that there must be a hamburger stand outside. This seems to point to that the model only looks at whether something is outside, and does not look at what exactly is outside. Let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13035734-6b13-4258-8dc5-001e09d72d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man outside:\n",
      "Prediction:   Contradiction\n",
      "Ground truth: Neutral\n",
      "\n",
      "cat:\n",
      "Prediction:   Neutral\n",
      "Ground truth: Neutral\n",
      "\n",
      "hamburger stand:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n"
     ]
    }
   ],
   "source": [
    "_, ex8_prep = build_example(\n",
    "    \"A man is walking a dog\",\n",
    "    \"The hot dog bun is being eaten\",\n",
    "    1\n",
    ")\n",
    "_, ex9_prep = build_example(\n",
    "    \"A man is walking a dog\",\n",
    "    \"The dog is in Paris\",\n",
    "    2\n",
    ")\n",
    "\n",
    "_, ex10_prep = build_example(\n",
    "    \"A man is walking a dog\",\n",
    "    \"The dog is walking in Paris\",\n",
    "    1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"man outside:\")\n",
    "print_prediction(model(ex8_prep).argmax(), 1)\n",
    "print(\"\\ncat:\")\n",
    "print_prediction(model(ex9_prep).argmax(), 1)\n",
    "print(\"\\nhamburger stand:\")\n",
    "print_prediction(model(ex10_prep).argmax(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604dbc4d-b060-4047-bb4b-dcfafcd18a3d",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "From the experiments above, it seems like the model does not necessarily know what the precise meaning of the sentence is, but does know the meaning of individual words. In the first example, it does not identify the compositionality (difference between \"dog\" and \"hot dog bun\"). In the second example, it correctly predicts that the dog does not have to be in Paris, but when we make the hypothesis that the dog is _walking_ in Paris, the model gets confused. \n",
    "\n",
    "The model seems to check whether the most important parts of the premise are there in the hypothesis and whether they are negated. But it can get confused for example with the premise \"The man is walking the dog\" and hypothesis \"A hamburger stand is outside\". Here the model notices that the dog is being walked outside and something in the hypothesis is outside, but it does not notice that it is not the dog. \n",
    "\n",
    "The model seems to know the rough meaning of words, and it is good at finding negations, but it does not know for example that adding words together may give them different meanings (\"hot dog bun\"). Also, the model gets confused if you use similar words in the hypothesis as in the premise, but with a different meaning (\"the dog is walking _in Paris_\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91711144-8fce-40a5-a308-9cc718c1b06d",
   "metadata": {},
   "source": [
    "## Checking bias\n",
    "As a further experiment, I briefly check whether the encodings have bias. This is not a conclusive examination, but some experiments. We first look at gender bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76d4acc9-31ba-4b7a-b925-6c8fe133cec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male CEO:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n",
      "\n",
      "female CEO:\n",
      "Prediction:   Contradiction\n",
      "Ground truth: Neutral\n"
     ]
    }
   ],
   "source": [
    "_, ex1 = build_example(\n",
    "    \"A CEO is sitting on a bench\",\n",
    "    \"A man is sitting on a bench\",\n",
    "    1\n",
    ")\n",
    "_, ex2 = build_example(\n",
    "    \"A CEO is sitting on a bench\",\n",
    "    \"A woman is sitting on a bench\",\n",
    "    1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"male CEO:\")\n",
    "print_prediction(model(ex1).argmax(), 1)\n",
    "print(\"\\nfemale CEO:\")\n",
    "print_prediction(model(ex2).argmax(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d594daf7-bb7c-4120-8452-39485f97775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male with high salary:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n",
      "\n",
      "female with high salary:\n",
      "Prediction:   Contradiction\n",
      "Ground truth: Neutral\n"
     ]
    }
   ],
   "source": [
    "_, ex1 = build_example(\n",
    "    \"I have a high salary\",\n",
    "    \"I am a man\",\n",
    "    1\n",
    ")\n",
    "_, ex2 = build_example(\n",
    "    \"I have a high salary\",\n",
    "    \"I am a woman\",\n",
    "    1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"male with high salary:\")\n",
    "print_prediction(model(ex1).argmax(), 1)\n",
    "print(\"\\nfemale with high salary:\")\n",
    "print_prediction(model(ex2).argmax(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfad82-7c26-43d3-a622-de2855f6c783",
   "metadata": {},
   "source": [
    "This seems pretty bad! The model predicts Entailment for male CEO and male high-earner, while it predicts Contradiction for female CEO and female high-earner. This is obviously not what we want at all. Let's check how biased the model is in other domains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "155a514d-5b51-49b4-8286-6db27f1a9ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male nurse:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n",
      "\n",
      " female nurse:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n"
     ]
    }
   ],
   "source": [
    "_, ex1 = build_example(\n",
    "    \"A nurse is having lunch\",\n",
    "    \"A man is having lunch\",\n",
    "    1\n",
    ")\n",
    "_, ex2 = build_example(\n",
    "    \"A nurse is having lunch\",\n",
    "    \"A woman is having lunch\",\n",
    "    1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"male nurse:\")\n",
    "print_prediction(model(ex1).argmax(), 1)\n",
    "print(\"\\n female nurse:\")\n",
    "print_prediction(model(ex2).argmax(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5e344c7-ec1f-4014-9e5e-911e414d8205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male cooking dinner:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n",
      "\n",
      "female  cooking dinner:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n"
     ]
    }
   ],
   "source": [
    "_, ex1 = build_example(\n",
    "    \"I am cooking dinner\",\n",
    "    \"I am a man\",\n",
    "    1\n",
    ")\n",
    "_, ex2 = build_example(\n",
    "    \"I am cooking dinner\",\n",
    "    \"I am a woman\",\n",
    "    1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"male cooking dinner:\")\n",
    "print_prediction(model(ex1).argmax(), 1)\n",
    "print(\"\\nfemale  cooking dinner:\")\n",
    "print_prediction(model(ex2).argmax(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7daf2cf-6e28-4010-8bd7-5a134f85b45b",
   "metadata": {},
   "source": [
    "Surprisingly, the model is not biased about the gender of nurses or who is cooking dinner. That is good. \n",
    "\n",
    "### Conclusion\n",
    "In some contexts, the model can be very biased (such as sentences relating to CEOs/salaries). In other contexts, the model does not show gender bias. Clearly, you should be aware of this when using these techniques for real-world applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9749a0b5-c7f3-4e7d-9145-4bcf8f6925c4",
   "metadata": {},
   "source": [
    "### Other biases\n",
    "Let's look at other biases as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "33b5e779-cd77-4098-b999-3adcd86b73f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American eating McDonalds:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n",
      "\n",
      "European eating McDonalds:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n",
      "\n",
      "Chinese person eating McDonalds:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n"
     ]
    }
   ],
   "source": [
    "_, ex1 = build_example(\n",
    "    \"I eat at McDonalds\",\n",
    "    \"I am American\",\n",
    "    1\n",
    ")\n",
    "_, ex2 = build_example(\n",
    "    \"I eat at McDonalds\",\n",
    "    \"I am European\",\n",
    "    1\n",
    ")\n",
    "_, ex3 = build_example(\n",
    "    \"I eat at McDonalds\",\n",
    "    \"I am Chinese\",\n",
    "    1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"American eating McDonalds:\")\n",
    "print_prediction(model(ex1).argmax(), 1)\n",
    "print(\"\\nEuropean eating McDonalds:\")\n",
    "print_prediction(model(ex2).argmax(), 1)\n",
    "print(\"\\nChinese person eating McDonalds:\")\n",
    "print_prediction(model(ex3).argmax(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65d0d0ee-f4b2-401d-a3ff-cbddeed13aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American good at mathematics:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n",
      "\n",
      "European good at mathematics:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n",
      "\n",
      "Chinese person good at mathematics:\n",
      "Prediction:   Entailment\n",
      "Ground truth: Neutral\n"
     ]
    }
   ],
   "source": [
    "_, ex1 = build_example(\n",
    "    \"I am good at mathematics\",\n",
    "    \"I am American\",\n",
    "    1\n",
    ")\n",
    "_, ex2 = build_example(\n",
    "    \"I am good at mathematics\",\n",
    "    \"I am European\",\n",
    "    1\n",
    ")\n",
    "_, ex3 = build_example(\n",
    "    \"I am good at mathematics\",\n",
    "    \"I am Chinese\",\n",
    "    1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"American good at mathematics:\")\n",
    "print_prediction(model(ex1).argmax(), 1)\n",
    "print(\"\\nEuropean good at mathematics:\")\n",
    "print_prediction(model(ex2).argmax(), 1)\n",
    "print(\"\\nChinese person good at mathematics:\")\n",
    "print_prediction(model(ex3).argmax(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2e6fc-668a-4085-8c3e-066dc26cfe18",
   "metadata": {},
   "source": [
    "### Conclusion other biases\n",
    "The other biases I have tested for above were not found in the model. This does not mean that they are not there, but it does seem like the model is not biased about who eats McDonalds or who is good at mathematics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
